# Asked ChatGPT
## What are terminal symbols?

In the context of formal grammars and language theory, terminal symbols are the basic units of a language that cannot be further divided within the context of the grammar. They are the leaf nodes of the syntactic tree, representing the fundamental building blocks or tokens of a language.

Terminal symbols are contrasted with non-terminal symbols, which can be expanded into other symbols. Non-terminals are placeholders for patterns that can be further decomposed according to the rules of the grammar.

In formal grammar notations such as Backus-Naur Form (BNF) or Extended Backus-Naur Form (EBNF), terminal symbols are often represented by literals, characters, or regular expressions. Here's a simple example in EBNF:

```ebnf
expression ::= term ('+' term)*
term       ::= NUMBER
NUMBER     ::= [0-9]+
```

In this example:

 `NUMBER` is a terminal symbol, representing one or more digits.

In the context of programming languages:

- Keywords, identifiers, literals (such as numbers or strings), and punctuation symbols are common examples of terminal symbols.
- In a programming language grammar, a terminal symbol could represent a specific keyword (e.g., `if`, `else`), an identifier (variable or function names), a literal value (e.g., `42`, `"hello"`), or a punctuation symbol (e.g., `{`, `}`, `;`).

In the syntactic analysis of a program, the lexer (or lexical analyzer) identifies and categorizes the input characters into terminal symbols. These terminal symbols are then used by the parser to build the syntactic structure of the program according to the rules specified in the grammar.

## What is a syntactic grammar?
A syntactic grammar, in the context of formal language theory and computer science, is a set of rules that defines the structure of valid sentences or programs in a language. It describes how symbols (or tokens) can be combined to form syntactically correct expressions, statements, or programs.

Syntactic grammars are used to specify the syntax of programming languages and other formal languages. They provide a formal and systematic way to express the rules for constructing valid sentences in a language. Two commonly used notations for representing syntactic grammars are Backus-Naur Form (BNF) and Extended Backus-Naur Form (EBNF).

Here's a simple example of a syntactic grammar for arithmetic expressions in EBNF:

```ebnf
expression ::= term ('+' term | '-' term)*
term       ::= factor ('*' factor | '/' factor)*
factor     ::= '(' expression ')' | NUMBER
NUMBER     ::= [0-9]+
```

In this example:

- `expression`, `term`, and `factor` are non-terminal symbols, representing higher-level constructs.
- `'+'`, `'-'`, `'*'`, `'/'`, `'('`, `')'`, and `NUMBER` are terminal symbols, representing basic units or tokens.
- `::=` is used to indicate a production, showing how non-terminals can be expanded.

This grammar describes how valid arithmetic expressions can be formed using addition, subtraction, multiplication, division, parentheses, and numerical values.

Syntactic grammars are crucial in the design and implementation of compilers and interpreters. Compiler tools like lexer (lexical analyzer) and parser generators use syntactic grammars to analyze the structure of source code and generate abstract syntax trees, which are then used for further processing and translation into executable code.


# What is a lexical token?
In the context of programming languages and compilers, a lexical token is a sequence of characters in the source code that represents a single, atomic unit of meaning. Lexical tokens are the basic building blocks of a programming language and are generated by the lexical analysis phase of the compilation process. This phase is often referred to as lexing or tokenization.

Each lexical token corresponds to a specific category of language elements, such as keywords, identifiers, literals, operators, and punctuation symbols. The process of lexing involves scanning the source code to recognize and categorize these tokens.

Here are some common types of lexical tokens:

1. **Keywords:** Reserved words that have a special meaning in the programming language. Examples include `if`, `else`, `while`, and `int`.

2. **Identifiers:** Names given to variables, functions, classes, or other user-defined entities. Examples include variable names like `count` or function names like `calculateTotal`.

3. **Literals:** Literal values representing fixed data. Examples include numeric literals (`42`), string literals (`"hello"`), and boolean literals (`true` or `false`).

4. **Operators:** Symbols representing operations. Examples include arithmetic operators (`+`, `-`, `*`, `/`), comparison operators (`==`, `!=`), and logical operators (`&&`, `||`).

5. **Punctuation Symbols:** Characters that provide structure to the code. Examples include parentheses `()`, braces `{}`, brackets `[]`, commas `,`, and semicolons `;`.

6. **Comments:** Text in the source code that is not meant for execution but provides information to developers. Comments are typically ignored during lexing.

Lexical tokens are the input for the subsequent phase of the compilation process known as parsing. The parser uses the tokens to build a syntactic structure, such as an abstract syntax tree (AST), which represents the hierarchical structure of the source code according to the language's grammar rules.

## What are lookahead restrictions?
In the context of parsing and compiler construction, lookahead restrictions refer to limitations on the number of tokens that a parser can examine ahead in the input stream before making a parsing decision. The "lookahead" is the number of tokens the parser peeks at to decide how to proceed with parsing.

Lookahead restrictions are often associated with certain parsing techniques and parsing algorithms. Here are a few common scenarios:

1. **LL(k) Parsing:**
   - In LL(k) parsing, the parser looks at the next k tokens in the input stream to decide which production rule to apply. The "LL" stands for "left-to-right, leftmost derivation," and the "k" represents the number of tokens of lookahead.
   - Lookahead restrictions in LL(k) parsers can limit the grammars they can handle. Larger values of k allow more complex grammars to be parsed, but they may increase the complexity of the parser.

2. **LL(1) Parsing:**
   - LL(1) parsing is a specific case where the parser uses only one token of lookahead. This simplifies the parsing process but imposes restrictions on the grammars that can be parsed.
   - The LL(1) restriction helps in making parsing decisions without ambiguity but may rule out certain types of grammars that require more lookahead.

3. **LR(k) Parsing:**
   - In LR(k) parsing, the parser looks at the next k tokens and uses this information to decide how to shift, reduce, or perform other parsing actions.
   - Similar to LL(k) parsing, larger values of k in LR(k) allow the parser to handle more complex grammars, but they also increase the computational complexity.

The choice of lookahead value is a trade-off between simplicity and expressive power. Smaller lookahead values simplify parsing algorithms but restrict the grammars that can be parsed. Larger lookahead values can handle more complex grammars but may result in more computationally intensive parsing algorithms.

Parser generators often allow specifying lookahead values, and the choice depends on the grammar and language being parsed. Some parsing algorithms, such as LALR(1) and SLR(1), aim to strike a balance between simplicity and expressive power in handling lookahead.
